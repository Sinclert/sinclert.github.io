---
title: "Why using Google Protocol Buffers"
layout: post
date: 2020-09-10 18:00
image: /assets/images-posts/protobuf-logo.png
headerImage: true
tag:
- software
- architecture
- data
category: blog
author: sinclert
description: "An introduction to Protocol Buffers its main benefits and use cases"
---


## Index

- [Introduction](#introduction)
- [Protobuf benefits](#protobuf-benefits)
- [Protobuf use case](#protobuf-use-case)
- [Repository hierarchy](#repository-hierarchy)
- [Models life cycle](#models-life-cycle-)
- [Public resources](#public-resources)
- [Summary](#summary)


## Introduction
Google _Protocol Buffers_, also called _Protobuf_, is a **language neutral,
platform neutral, extensible** dialect for describing data models. However,
it is way more than this. Protobuf encapsulates a range of tools for serializing
data objects so that they can be interchanged between different software components.

**Something like [JSON-schema][json-schema-web], but on steroids**.

This post does not cover the [Protobuf syntax][protobuf-syntax] (currently at v3),
but instead explains the reasons why to use it.


## Protobuf benefits
Given the unique properties of Protobuf, it serves as a great solution for defining
data schemas that several software components are going to share, where some of them
may be **written in different programming languages**.

In addition, there are other benefits that come into play.

### Language support üí± 
As explained before, Protobuf offers both a data model describing dialect, and a
set of _encoders_ and _decoders_ to serialize and deserialize shared data objects.

These _encoders_ and _decoders_ are bundled with the Protobuf libraries, and they have been
created for supporting a wide variety of programming languages: C++, Python, Java, Ruby, 
Golang, JavaScript, PHP... The complete list of supported languages can be checked
on its [GitHub repository][protobuf-repo].

They are in charge of **automatically generate language-specific classes**,
where the serialized data objects can be deserialized into. This is a great advantage
over _JSON-schema_, as _Protobuf generated_ classes can be used by IDEs and linters
to statically check your code.

Simplified example:

```protobuf
// Protobuf model                           # Python class (generated)
message Ingredient {                        class Ingredient:
  string id = 1;                                id: str
  string name = 2;                              name: str
  int32 healthiness = 3;                        healthiness: int
}
```

### Speed ‚ö°Ô∏è
Serialization speed is another area where _Protobuf_ excels, with the following speed-ups, compared to JSON:

- **On JS-to-Java:** ~20%. [Reference][performance-post-java].
- **On Java-to-Java:** ~80% (5-6 times faster). [Reference][performance-post-java].
- **On Golang-to-Golang:** ~40% (2.3-2.7 times faster). [Reference][performance-post-golang].

I do not consider this to be a huge benefit, as data serialization is usually not
the bottleneck of modern days applications. Just something to keep in mind.

### Customization üé®
Protobuf offers _default_ translations of `.proto` models to each particular programming
language class equivalent. Nevertheless, it is flexible enough to allow developers create
their own translations if they want to use modern features of those programming languages.

My favourite example is [betterproto][better-proto-github], a Protobuf to Python plugin
that simplifies the resulting classes, and makes use of the `dataclass` decorator.

If the default translation is called:

```shell
protoc --proto_path=$SRC_DIR --python_out=$DST_DIR $SRC_DIR/ingredient.proto
```

A customized translation is called:

```shell
protoc --proto_path=$SRC_DIR --python_betterproto_out=$DST_DIR $SRC_DIR/ingredient.proto
```


## Protobuf use case
Overall, _Protocol Buffers_ are useful on quite particular scenarios, that are only
reached once an organization has a certain size, as it addresses a scalability problem by
creating a **single source of truth** on how shared data models are defined and evolved over time.

Here is a real example:

_Your company now has 50+ software engineers, and you are starting to migrate to a micro-services
architecture where some are written in Python, others in Golang, and others in Ruby.
How do you make sure data schemas shared among some of those microservices are consistent?_

It is probable that Golang schemas evolve faster than the Ruby ones, and the required coordination
among teams to ensure they are introducing the same new fields is slowing down the development.

That is where a **centralized repository of Protobuf schemas** helps.


## Repository hierarchy
As introduced in the previous section, using Protobuf schemas become useful when they
are defined on a repository representing the _single source of truth_ within an organization.

That repository of Protobuf schemas hold the data models that teams managing components written
in different programming languages are going to use. Therefore, they will need some kind of
automatic translation from `.proto` to `.py`, `.go`, `.rb`...

That is when **repository hierarchy** comes into play.

The desired hierarchy would have an _upstream_ repository called `org-schemas`
with the Protobuf definitions, and a range of _downstream_ repositories called:
- `org-schemas-python` üêç
- `org-schemas-golang` üêá
- `org-schemas-ruby` üíé
- ...

So that every time a change is introduced on the Protobuf schemas, the [CI/CD][redhat-ci-cd] automatically:
1. Downloads a protoc compiler.
2. Generate the target language classes.
3. Clones and updates the target language repository.
4. Releases / tags the target language repository.


## Models life-cycle ‚ôªÔ∏è
With a hierarchical structure of repositories, data models evolution is propagated from the
central repo of Protobuf schemas to the downstream, language-specific, schemas repositories.

This approach automatically synchronize the state of data schemas across the different
programming languages that an organization may decide to use, so schemas at version `X.Y.Z`
of `org-schemas-python` repo have **exactly** the same fields as those in version `X.Y.Z` 
of `org-schemas-golang` repo.

Quite satisfying üòå

Now, there is a small catch: as automatically detecting what is considered a _major_, _minor_ or _patch_
change by the Protobuf repository CI is not easy, [semantic versioning][semantic-versioning] format
cannot be followed on the downstream repositories.

That means that versions will not be `X.Y.Z` (i.e. `2.1.0`), but `X` (i.e. `12`),
being incremented with each new change.


## Public resources

### GitHub skeleton
In order to facilitate the understanding and the setup of this structure of repositories
(which lays more in the _infrastructure_ than in the _development_ part of software), I made
a public repository with the skeleton of how the upstream _Protobuf holding_ repo would look like.

Check it out: [Sinclert/Protobuf-upstream][protobuf-skeleton-repo].

### Additional considerations
- A pair of RSA keys is required for connecting upstream and downstream repos.
    - The private part is set as _"Secret"_ within the upstream repository.
    - The public part is set as _"Deploy key"_ within the downstream repositories.

- **Advance usage:** if the resulting language-specific schema repositories want to be
kept secret within an organization, but being used at Dockerized pieces of software,
check out this fantastic [Docker SSH forwarding guide][docker-ssh-guide] on how
to use `--mount=type=ssh` to access those.


## Summary
Setting up a hierarchy of repositories to translate _language-neutral_ schemas into
_language-specific_ classes is challenging, but it also provides fantastic benefits
to big enough organizations:

- Single source of truth.
- Controlled evolution of downstream repositories.
- Language-specific generated classes (compared to JSON-schema).
- Language flexibility to choose from (C++, Python, Golang, Ruby, JS...).

Thanks for reading!

Feel free to contact me if you find any bugs on the skeleton repo üòâ


[better-proto-github]: https://github.com/danielgtaylor/python-betterproto
[docker-ssh-guide]: https://medium.com/@tonistiigi/build-secrets-and-ssh-forwarding-in-docker-18-09-ae8161d066
[json-schema-web]: https://json-schema.org
[performance-post-golang]: https://blog.usejournal.com/what-the-hell-is-protobuf-4aff084c5db4
[performance-post-java]: https://auth0.com/blog/beating-json-performance-with-protobuf/
[protobuf-skeleton-repo]: https://github.com/Sinclert/Protobuf-upstream
[protobuf-syntax]: https://developers.google.com/protocol-buffers/docs/proto3
[protobuf-repo]: https://github.com/protocolbuffers/protobuf
[redhat-ci-cd]: https://www.redhat.com/en/topics/devops/what-is-ci-cd
[semantic-versioning]: https://semver.org
